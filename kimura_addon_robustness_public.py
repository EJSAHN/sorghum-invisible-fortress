#!/usr/bin/env python3
"""
kimura_addon_robustness_public.py

Post-hoc robustness add-on for the Kimura sorghum hyperspectral pipeline.

What it does (no new wet-lab data required):
1) Re-states SDI construction from the exported phenotype table.
2) Computes SDI â†” disease associations under multiple confounding controls:
   - unadjusted
   - adjusted for genotype PCs
   - adjusted for race (categorical) + genotype PCs
3) Within-race association + meta-style summaries
4) Permutation test that preserves race structure (SDI shuffled within race)
5) SDI threshold sensitivity (e.g., SDI > 7 "Trap" threshold) and data-driven best threshold scan
6) Correlations between SDI and "physical proxy" columns if present (mass/size/weight/density/etc.)

Inputs expected (generated by kimura_pipeline_public.py):
- <out>/Phenotypes_with_Spectral_PCs.csv

Outputs (written to <out>/addons_robustness/):
- sdi_disease_models.csv
- sdi_disease_within_group.csv
- sdi_permutation_test.csv
- sdi_threshold_scan.csv
- sdi_physical_proxy_correlations.csv
- (optional) plots/*.pdf

This script is designed to be "GitHub clean":
- no hard-coded personal directories
- CLI arguments only
- no emojis / no non-ASCII commentary
"""

from __future__ import annotations

import argparse
import math
import os
import re
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Sequence, Tuple

import numpy as np
import pandas as pd
from scipy import stats
import matplotlib.pyplot as plt


# -----------------------------
# Small stats helpers (no statsmodels dependency)
# -----------------------------

def _as_2d(x: np.ndarray) -> np.ndarray:
    x = np.asarray(x)
    if x.ndim == 1:
        return x.reshape(-1, 1)
    return x

def ols_fit(y: np.ndarray, X: np.ndarray) -> Dict[str, np.ndarray | float | int]:
    """
    Ordinary least squares with intercept already included in X (if desired).

    Returns:
        beta, se, t, p, r2, df_resid, n
    """
    y = np.asarray(y).astype(float)
    X = np.asarray(X).astype(float)

    # Drop rows with any NaN/Inf
    ok = np.isfinite(y) & np.all(np.isfinite(X), axis=1)
    y = y[ok]
    X = X[ok]

    n, p = X.shape
    if n <= p:
        raise ValueError(f"Not enough samples after filtering: n={n}, p={p}")

    XtX = X.T @ X
    try:
        XtX_inv = np.linalg.inv(XtX)
    except np.linalg.LinAlgError:
        XtX_inv = np.linalg.pinv(XtX)

    beta = XtX_inv @ (X.T @ y)
    yhat = X @ beta
    resid = y - yhat

    df = n - p
    rss = float(resid.T @ resid)
    tss = float(((y - y.mean()) ** 2).sum())
    r2 = 1.0 - rss / tss if tss > 0 else float("nan")

    sigma2 = rss / df
    cov_beta = sigma2 * XtX_inv
    se = np.sqrt(np.clip(np.diag(cov_beta), 0, np.inf))

    tvals = beta / se
    pvals = 2.0 * stats.t.sf(np.abs(tvals), df)

    return {
        "beta": beta,
        "se": se,
        "t": tvals,
        "p": pvals,
        "r2": r2,
        "df_resid": int(df),
        "n": int(n),
        "rss": rss,
    }

def partial_r2(sse_reduced: float, sse_full: float) -> float:
    """Partial R^2 for adding a block of predictors."""
    if sse_reduced <= 0:
        return float("nan")
    return max(0.0, (sse_reduced - sse_full) / sse_reduced)

def design_matrix(
    df: pd.DataFrame,
    *,
    sdi_col: str,
    pcs: Sequence[str],
    race_col: Optional[str],
    include_sdi: bool,
    add_intercept: bool = True,
) -> Tuple[np.ndarray, List[str]]:
    """
    Build X matrix with optional SDI, optional PCs, optional race one-hot encoding.
    """
    parts: List[np.ndarray] = []
    names: List[str] = []

    if add_intercept:
        parts.append(np.ones((len(df), 1)))
        names.append("Intercept")

    if include_sdi:
        parts.append(_as_2d(df[sdi_col].to_numpy(dtype=float)))
        names.append(sdi_col)

    for pc in pcs:
        parts.append(_as_2d(df[pc].to_numpy(dtype=float)))
        names.append(pc)

    if race_col is not None and race_col in df.columns:
        # One-hot encode, drop first level to avoid collinearity
        dummies = pd.get_dummies(df[race_col].astype(str), prefix=race_col, drop_first=True)
        if dummies.shape[1] > 0:
            parts.append(dummies.to_numpy(dtype=float))
            names.extend(dummies.columns.tolist())

    X = np.concatenate(parts, axis=1) if parts else np.empty((len(df), 0))
    return X, names

def find_candidate_traits(df: pd.DataFrame) -> List[str]:
    """
    Heuristic to find disease severity traits.
    You can override with --traits.
    """
    candidates = []
    # Common names used in the pipeline/manuscript
    hard = [
        "headsmut_highest_score",
        "headsmut_greenhouse_avg",
        "headsmut_field_score",
        "headsmut_score",
        "head_smut",
        "smut",
        "severity",
        "disease",
    ]
    for c in df.columns:
        cl = c.lower()
        if any(h in cl for h in hard):
            # Avoid picking binary/categorical covariates by accident
            if df[c].dtype.kind in "iuf":
                candidates.append(c)
    # De-duplicate preserving order
    seen = set()
    out = []
    for c in candidates:
        if c not in seen:
            out.append(c); seen.add(c)
    return out

def find_pc_columns(df: pd.DataFrame) -> List[str]:
    pcs = [c for c in df.columns if re.fullmatch(r"PC\d+", str(c))]
    # sort by integer index
    pcs.sort(key=lambda x: int(x[2:]))
    return pcs

def find_race_column(df: pd.DataFrame) -> Optional[str]:
    for name in ["race", "Race", "subpopulation", "population", "pop", "group"]:
        if name in df.columns:
            return name
    return None

def find_physical_proxy_columns(df: pd.DataFrame) -> List[str]:
    patterns = [
        r"mass", r"weight", r"density", r"volume", r"size", r"area",
        r"length", r"width", r"height", r"thickness", r"geometry", r"hydration", r"water"
    ]
    cols = []
    for c in df.columns:
        cl = str(c).lower()
        if any(re.search(p, cl) for p in patterns) and df[c].dtype.kind in "iuf":
            cols.append(c)
    # remove SDI/PC columns
    cols = [c for c in cols if not (c.startswith("PC") or c == "SDI")]
    # de-duplicate
    seen = set()
    out = []
    for c in cols:
        if c not in seen:
            out.append(c); seen.add(c)
    return out


# -----------------------------
# Main analysis
# -----------------------------

def run_models(
    df: pd.DataFrame,
    *,
    sdi_col: str,
    trait_cols: Sequence[str],
    pc_cols: Sequence[str],
    n_pcs: int,
    race_col: Optional[str],
) -> pd.DataFrame:
    pcs = list(pc_cols[:n_pcs]) if n_pcs > 0 else []
    rows = []

    for trait in trait_cols:
        y = df[trait].to_numpy(dtype=float)

        # Reduced model(s)
        X0, names0 = design_matrix(df, sdi_col=sdi_col, pcs=[], race_col=None, include_sdi=False)
        fit0 = ols_fit(y, X0)  # intercept-only

        X_cov, names_cov = design_matrix(df, sdi_col=sdi_col, pcs=pcs, race_col=race_col, include_sdi=False)
        fit_cov = ols_fit(y, X_cov)

        # Full models with SDI
        X_sdi, names_sdi = design_matrix(df, sdi_col=sdi_col, pcs=[], race_col=None, include_sdi=True)
        fit_sdi = ols_fit(y, X_sdi)

        X_full, names_full = design_matrix(df, sdi_col=sdi_col, pcs=pcs, race_col=race_col, include_sdi=True)
        fit_full = ols_fit(y, X_full)

        # Identify SDI coefficient index in the full model
        sdi_idx = names_full.index(sdi_col)

        rows.append({
            "trait": trait,
            "n": fit_full["n"],
            "n_pcs": n_pcs,
            "race_col": race_col or "",
            # Unadjusted
            "beta_sdi_unadj": float(fit_sdi["beta"][names_sdi.index(sdi_col)]),
            "p_sdi_unadj": float(fit_sdi["p"][names_sdi.index(sdi_col)]),
            "r2_unadj": float(fit_sdi["r2"]),
            # Adjusted
            "beta_sdi_adj": float(fit_full["beta"][sdi_idx]),
            "p_sdi_adj": float(fit_full["p"][sdi_idx]),
            "t_sdi_adj": float(fit_full["t"][sdi_idx]),
            "r2_adj": float(fit_full["r2"]),
            "partial_r2_sdi": partial_r2(float(fit_cov["rss"]), float(fit_full["rss"])),
        })

    return pd.DataFrame(rows)

def within_group_models(
    df: pd.DataFrame,
    *,
    group_col: str,
    sdi_col: str,
    trait_cols: Sequence[str],
    pc_cols: Sequence[str],
    n_pcs: int,
) -> pd.DataFrame:
    pcs = list(pc_cols[:n_pcs]) if n_pcs > 0 else []
    out_rows = []

    for group, sub in df.groupby(group_col):
        if len(sub) < max(10, 3 + n_pcs + 1):
            continue
        for trait in trait_cols:
            y = sub[trait].to_numpy(dtype=float)
            X, names = design_matrix(sub, sdi_col=sdi_col, pcs=pcs, race_col=None, include_sdi=True)
            fit = ols_fit(y, X)
            sdi_idx = names.index(sdi_col)
            out_rows.append({
                "group_col": group_col,
                "group": str(group),
                "trait": trait,
                "n": fit["n"],
                "n_pcs": n_pcs,
                "beta_sdi": float(fit["beta"][sdi_idx]),
                "p_sdi": float(fit["p"][sdi_idx]),
                "r2": float(fit["r2"]),
            })
    return pd.DataFrame(out_rows)

def permutation_test(
    df: pd.DataFrame,
    *,
    sdi_col: str,
    trait: str,
    pc_cols: Sequence[str],
    n_pcs: int,
    race_col: Optional[str],
    n_perm: int,
    seed: int,
) -> Dict[str, float]:
    rng = np.random.default_rng(seed)
    pcs = list(pc_cols[:n_pcs]) if n_pcs > 0 else []
    y = df[trait].to_numpy(dtype=float)

    X_full, names_full = design_matrix(df, sdi_col=sdi_col, pcs=pcs, race_col=race_col, include_sdi=True)
    fit_obs = ols_fit(y, X_full)
    sdi_idx = names_full.index(sdi_col)
    t_obs = float(fit_obs["t"][sdi_idx])

    # Prepare grouping
    if race_col is not None and race_col in df.columns:
        groups = df[race_col].astype(str).to_numpy()
    else:
        groups = np.array(["all"] * len(df), dtype=object)

    sdi = df[sdi_col].to_numpy(dtype=float)
    t_perm = np.empty(n_perm, dtype=float)

    # Pre-build covariate matrix WITHOUT SDI for efficiency? We still need full X.
    # We'll just replace the SDI column in X_full each iteration.
    X_work = X_full.copy()

    for i in range(n_perm):
        sdi_shuf = sdi.copy()
        for g in np.unique(groups):
            idx = np.where(groups == g)[0]
            if len(idx) <= 1:
                continue
            rng.shuffle(sdi_shuf[idx])
        X_work[:, sdi_idx] = sdi_shuf
        fit = ols_fit(y, X_work)
        t_perm[i] = float(fit["t"][sdi_idx])

    p_emp = (np.sum(np.abs(t_perm) >= abs(t_obs)) + 1.0) / (n_perm + 1.0)

    return {
        "trait": trait,
        "n": int(fit_obs["n"]),
        "n_pcs": n_pcs,
        "race_col": race_col or "",
        "t_obs": t_obs,
        "p_parametric": float(fit_obs["p"][sdi_idx]),
        "p_empirical": float(p_emp),
    }

def threshold_scan(
    df: pd.DataFrame,
    *,
    sdi_col: str,
    trait: str,
    candidate_thresholds: Sequence[float],
) -> pd.DataFrame:
    y = df[trait].to_numpy(dtype=float)
    sdi = df[sdi_col].to_numpy(dtype=float)

    rows = []
    for thr in candidate_thresholds:
        hi = sdi > thr
        lo = ~hi
        if hi.sum() < 5 or lo.sum() < 5:
            continue
        tstat, p = stats.ttest_ind(y[hi], y[lo], equal_var=False, nan_policy="omit")
        rows.append({
            "trait": trait,
            "threshold": float(thr),
            "n_hi": int(hi.sum()),
            "n_lo": int(lo.sum()),
            "mean_hi": float(np.nanmean(y[hi])),
            "mean_lo": float(np.nanmean(y[lo])),
            "delta_hi_minus_lo": float(np.nanmean(y[hi]) - np.nanmean(y[lo])),
            "t_stat": float(tstat),
            "p_value": float(p),
        })
    out = pd.DataFrame(rows).sort_values("p_value", ascending=True)
    return out

def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--out", type=str, default="outputs", help="Output folder from kimura_pipeline_public.py")
    ap.add_argument("--sdi_col", type=str, default="SDI", help="SDI column name in phenotype table")
    ap.add_argument("--traits", type=str, default="", help="Comma-separated list of disease trait columns (optional)")
    ap.add_argument("--n_pcs", type=int, default=3, help="Number of genotype PCs to use as covariates")
    ap.add_argument("--race_col", type=str, default="", help="Race/group column name (optional, autodetect if empty)")
    ap.add_argument("--n_perm", type=int, default=2000, help="Permutations for empirical p-value")
    ap.add_argument("--perm_seed", type=int, default=123, help="Random seed for permutations")
    ap.add_argument("--trap_threshold", type=float, default=7.0, help="Specific SDI threshold to report (e.g., 7)")
    ap.add_argument("--make_plots", action="store_true", help="Write basic PDF plots")
    args = ap.parse_args()

    out_root = Path(args.out)
    pheno_path = out_root / "Phenotypes_with_Spectral_PCs.csv"
    if not pheno_path.exists():
        raise FileNotFoundError(f"Missing {pheno_path}. Run kimura_pipeline_public.py first.")

    df = pd.read_csv(pheno_path)

    if args.sdi_col not in df.columns:
        raise ValueError(f"SDI column '{args.sdi_col}' not found in {pheno_path.name}")

    pc_cols = find_pc_columns(df)
    if args.n_pcs > 0 and len(pc_cols) < args.n_pcs:
        raise ValueError(f"Requested n_pcs={args.n_pcs} but only found PCs: {pc_cols}")

    race_col = args.race_col.strip() or (find_race_column(df) or "")
    race_col = race_col if race_col in df.columns else ""

    if args.traits.strip():
        trait_cols = [t.strip() for t in args.traits.split(",") if t.strip()]
        missing = [t for t in trait_cols if t not in df.columns]
        if missing:
            raise ValueError(f"Trait columns not found: {missing}")
    else:
        trait_cols = find_candidate_traits(df)
        if not trait_cols:
            raise ValueError("Could not auto-detect trait columns. Provide --traits.")

    addon_dir = out_root / "addons_robustness"
    plot_dir = addon_dir / "plots"
    addon_dir.mkdir(parents=True, exist_ok=True)
    plot_dir.mkdir(parents=True, exist_ok=True)

    # 1) Main models
    models_df = run_models(
        df,
        sdi_col=args.sdi_col,
        trait_cols=trait_cols,
        pc_cols=pc_cols,
        n_pcs=args.n_pcs,
        race_col=race_col or None,
    )
    models_df.to_csv(addon_dir / "sdi_disease_models.csv", index=False)

    # 2) Within-group
    if race_col:
        within_df = within_group_models(
            df,
            group_col=race_col,
            sdi_col=args.sdi_col,
            trait_cols=trait_cols,
            pc_cols=pc_cols,
            n_pcs=args.n_pcs,
        )
        within_df.to_csv(addon_dir / "sdi_disease_within_group.csv", index=False)
    else:
        within_df = pd.DataFrame()

    # 3) Permutation tests (one per trait)
    perm_rows = []
    for tr in trait_cols:
        perm_rows.append(permutation_test(
            df,
            sdi_col=args.sdi_col,
            trait=tr,
            pc_cols=pc_cols,
            n_pcs=args.n_pcs,
            race_col=(race_col or None),
            n_perm=args.n_perm,
            seed=args.perm_seed,
        ))
    perm_df = pd.DataFrame(perm_rows)
    perm_df.to_csv(addon_dir / "sdi_permutation_test.csv", index=False)

    # 4) Threshold scan (quantile grid + explicit trap threshold)
    sdi_vals = df[args.sdi_col].to_numpy(dtype=float)
    quantiles = np.unique(np.quantile(sdi_vals[np.isfinite(sdi_vals)], np.linspace(0.5, 0.99, 100)))
    candidates = sorted(set([float(args.trap_threshold)] + [float(q) for q in quantiles]))
    thr_rows = []
    for tr in trait_cols:
        thr_df = threshold_scan(df, sdi_col=args.sdi_col, trait=tr, candidate_thresholds=candidates)
        if len(thr_df) == 0:
            continue
        thr_df["trap_threshold"] = float(args.trap_threshold)
        thr_rows.append(thr_df)
    if thr_rows:
        thr_all = pd.concat(thr_rows, ignore_index=True)
        thr_all.to_csv(addon_dir / "sdi_threshold_scan.csv", index=False)
    else:
        thr_all = pd.DataFrame()

    # 5) Physical proxy correlations (if any)
    phys_cols = find_physical_proxy_columns(df)
    phys_rows = []
    for c in phys_cols:
        x = df[args.sdi_col].to_numpy(dtype=float)
        y = df[c].to_numpy(dtype=float)
        ok = np.isfinite(x) & np.isfinite(y)
        if ok.sum() < 10:
            continue
        r, p = stats.pearsonr(x[ok], y[ok])
        phys_rows.append({"proxy": c, "r_pearson": float(r), "p_pearson": float(p), "n": int(ok.sum())})
    phys_df = pd.DataFrame(phys_rows).sort_values("p_pearson", ascending=True) if phys_rows else pd.DataFrame()
    phys_df.to_csv(addon_dir / "sdi_physical_proxy_correlations.csv", index=False)

    # 6) Plots (optional)
    if args.make_plots:
        for tr in trait_cols:
            fig = plt.figure()
            ax = fig.add_subplot(111)
            ax.scatter(df[args.sdi_col], df[tr], s=12)
            ax.set_xlabel(args.sdi_col)
            ax.set_ylabel(tr)
            ax.set_title(f"{tr} vs {args.sdi_col}")
            fig.tight_layout()
            fig.savefig(plot_dir / f"scatter_{tr}_vs_{args.sdi_col}.pdf")
            plt.close(fig)

        if not thr_all.empty:
            for tr in trait_cols:
                sub = thr_all[thr_all["trait"] == tr].copy()
                if sub.empty:
                    continue
                fig = plt.figure()
                ax = fig.add_subplot(111)
                ax.plot(sub["threshold"].to_numpy(), sub["p_value"].to_numpy(), linewidth=1)
                ax.axvline(float(args.trap_threshold), linestyle="--")
                ax.set_xlabel("SDI threshold")
                ax.set_ylabel("Welch t-test p-value (hi vs lo)")
                ax.set_title(f"Threshold sensitivity: {tr}")
                ax.set_yscale("log")
                fig.tight_layout()
                fig.savefig(plot_dir / f"threshold_scan_{tr}.pdf")
                plt.close(fig)

    print(f"[OK] Wrote add-on outputs to: {addon_dir}")

if __name__ == "__main__":
    main()
