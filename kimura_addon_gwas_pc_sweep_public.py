#!/usr/bin/env python3
"""
kimura_addon_gwas_pc_sweep_public.py

Add-on to quantify sensitivity of GWAS results to population-structure controls:
- number of genotype PCs (0..K)
- optional race dummy covariates (if present)

This directly supports reviewer-facing robustness statements like:
"lead locus remains significant under 0-10 PCs and with/without race covariates"
and provides lambda_GC per model.

Inputs expected (generated by kimura_pipeline_public.py):
- <out>/Phenotypes_with_Spectral_PCs.csv
- <out>/cache/geno_dosage_<tag>.npy
- <out>/cache/snp_meta_<tag>.csv
- <out>/cache/geno_samples_<tag>.txt

Outputs written to <out>/addons_pc_sweep/:
- gwas_pc_sweep_summary.csv
- (optional) GWAS tables per configuration (if --save_full)

Note:
This script does NOT replace your main pipeline. It reuses the cached genotype matrix
and recomputes GWAS under alternative covariate sets.
"""

from __future__ import annotations

import argparse
import glob
import os
import re
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
from scipy import stats


MAF_MIN = 0.05
MISS_MAX = 0.10


def _lambda_gc_from_p(pvals: np.ndarray) -> float:
    p = np.clip(np.asarray(pvals, float), 1e-300, 1.0)
    chi2 = stats.chi2.isf(p, df=1)
    return float(np.median(chi2) / 0.4549364)


def _load_cached(cache_dir: Path, n_expected: int) -> Tuple[Path, Path, Path]:
    """
    Find cached genotype trio: npy + meta.csv + samples.txt
    The public pipeline writes:
      geno_dosage_<tag>.npy
      snp_meta_<tag>.csv
      geno_samples_<tag>.txt
    where tag includes _n{n}.
    """
    pat = f"*n{n_expected}*.npy"
    cand = sorted(cache_dir.glob(f"geno_dosage_{pat}"))
    if not cand:
        # fallback: any geno_dosage file
        cand = sorted(cache_dir.glob("geno_dosage_*.npy"))
        if not cand:
            raise FileNotFoundError(f"No cached genotype npy found under: {cache_dir}")

    npy_path = cand[-1]
    tag = npy_path.stem.replace("geno_dosage_", "")
    meta_path = cache_dir / f"snp_meta_{tag}.csv"
    samp_path = cache_dir / f"geno_samples_{tag}.txt"

    if not meta_path.exists() or not samp_path.exists():
        # try to infer tag from available files
        meta_cand = sorted(cache_dir.glob("snp_meta_*.csv"))
        samp_cand = sorted(cache_dir.glob("geno_samples_*.txt"))
        if not meta_cand or not samp_cand:
            raise FileNotFoundError(f"Cache incomplete under {cache_dir}: need snp_meta_*.csv and geno_samples_*.txt")
        meta_path = meta_cand[-1]
        samp_path = samp_cand[-1]

    return npy_path, meta_path, samp_path


def geno_qc(G_int8: np.ndarray, meta: pd.DataFrame, maf_min=MAF_MIN, miss_max=MISS_MAX) -> Tuple[np.ndarray, pd.DataFrame]:
    """
    Match public pipeline QC:
      - missing rate <= miss_max
      - MAF >= maf_min
      - chr/pos parseable
    """
    miss = np.mean(G_int8 < 0, axis=0)
    X = G_int8.astype(np.float32)
    X[X < 0] = np.nan
    p = np.nanmean(X, axis=0) / 2.0
    maf = np.minimum(p, 1 - p)

    chr_ok = pd.to_numeric(meta["chr"], errors="coerce").notna().to_numpy()
    pos_ok = pd.to_numeric(meta["pos"], errors="coerce").notna().to_numpy()

    valid = np.isfinite(maf) & (maf >= maf_min) & (miss <= miss_max) & chr_ok & pos_ok

    meta_q = meta.loc[valid].copy().reset_index(drop=True)
    meta_q["maf"] = maf[valid]
    meta_q["miss"] = miss[valid]

    G_q = G_int8[:, valid]
    return G_q, meta_q


def gwas_ols(y: np.ndarray, G_int8: np.ndarray, meta: pd.DataFrame, cov: Optional[np.ndarray], block: int = 20000) -> Tuple[pd.DataFrame, float, Dict]:
    """
    Copy of kimura_pipeline_public.py gwas_ols (OLS GWAS w/ covariates).
    """
    y = np.asarray(y, float)
    n0 = len(y)
    if cov is not None:
        cov = np.asarray(cov, float)
        if cov.shape[0] != n0:
            raise ValueError("cov rows do not match y length")

    msk = np.isfinite(y)
    if cov is not None:
        msk = msk & np.all(np.isfinite(cov), axis=1)

    y = y[msk]
    G = G_int8[msk, :]
    n = len(y)

    X = np.ones((n, 1), float) if cov is None else np.column_stack([np.ones((n, 1), float), cov[msk, :]])

    y = (y - np.mean(y)) / (np.std(y, ddof=0) + 1e-12)
    beta_y = np.linalg.lstsq(X, y, rcond=None)[0]
    y_res = y - X @ beta_y

    XtX_inv = np.linalg.inv(X.T @ X)
    Xt = X.T
    df = max(1, n - X.shape[1] - 1)
    ssy = float(np.sum(y_res * y_res))

    m = G.shape[1]
    betas = np.empty(m, float)
    pvals = np.empty(m, float)

    for s in range(0, m, block):
        e = min(m, s + block)
        Gb = G[:, s:e].astype(np.float32)
        Gb[Gb < 0] = np.nan
        mu = np.nanmean(Gb, axis=0)
        nan_idx = np.where(~np.isfinite(Gb))
        if nan_idx[0].size:
            Gb[nan_idx] = np.take(mu, nan_idx[1])
        Gb = Gb - mu
        sd = Gb.std(axis=0, ddof=0)
        sd[sd == 0] = 1.0
        Gb = Gb / sd

        B = XtX_inv @ (Xt @ Gb)
        Gr = Gb - X @ B
        num = Gr.T @ y_res
        den = np.sum(Gr * Gr, axis=0)
        den[den == 0] = np.nan
        b = num / den
        rss = np.maximum(ssy - b * num, 1e-30)
        se = np.sqrt((rss / df) / den)
        t = b / se
        p = 2.0 * stats.t.sf(np.abs(t), df=df)
        betas[s:e] = b
        pvals[s:e] = p

    out = meta.copy()
    out["beta"] = betas
    out["p"] = np.clip(pvals, 1e-300, 1.0)
    out["-log10p"] = -np.log10(out["p"].values)

    lam = _lambda_gc_from_p(out["p"].values)
    lead_i = int(out["-log10p"].values.argmax())
    lead = out.iloc[lead_i][["chr", "pos", "rs", "-log10p"]].to_dict() if "rs" in out.columns else out.iloc[lead_i][["chr", "pos", "-log10p"]].to_dict()
    return out, lam, lead


def _guess_accession_col(df: pd.DataFrame) -> str:
    for c in ["accession", "Accession", "sample", "Sample", "id", "ID"]:
        if c in df.columns:
            return c
    # fall back to first column
    return df.columns[0]


def _build_cov(df: pd.DataFrame, npcs: int, include_race: bool) -> Optional[np.ndarray]:
    cols: List[str] = []
    for i in range(npcs):
        c = f"PC{i+1}"
        if c in df.columns:
            cols.append(c)

    if include_race:
        cols += [c for c in df.columns if str(c).startswith("race_")]

    if not cols:
        return None
    return df[cols].to_numpy(dtype=float)


def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--out", type=str, default="outputs", help="Output folder from kimura_pipeline_public.py")
    ap.add_argument("--trait", type=str, default="SDI",
                    help="Phenotype column to GWAS (default SDI). You can also use Vis_PC1, NIR_PC1, or a disease trait.")
    ap.add_argument("--pc_list", type=str, default="0,1,2,3,4,6,8,10", help="Comma-separated PC counts to test")
    ap.add_argument("--include_race", type=str, default="0,1",
                    help="Comma-separated booleans: 0/1. Example: '0,1' to run w/ and w/o race covariates.")
    ap.add_argument("--save_full", action="store_true", help="Save GWAS tables for each configuration")
    ap.add_argument("--block", type=int, default=20000, help="SNP block size for GWAS")
    args = ap.parse_args()

    out_root = Path(args.out)
    pheno_path = out_root / "Phenotypes_with_Spectral_PCs.csv"
    if not pheno_path.exists():
        raise FileNotFoundError(f"Missing {pheno_path}. Run kimura_pipeline_public.py first.")

    dfp = pd.read_csv(pheno_path)
    acc_col = _guess_accession_col(dfp)
    dfp[acc_col] = dfp[acc_col].astype(str).str.strip()
    dfp = dfp.set_index(acc_col)

    if args.trait not in dfp.columns:
        raise ValueError(f"Trait '{args.trait}' not found in phenotype table columns.")

    cache_dir = out_root / "cache"
    if not cache_dir.exists():
        raise FileNotFoundError(f"Missing cache dir: {cache_dir}. Run kimura_pipeline_public.py first.")

    n_expected = len(dfp)
    npy_path, meta_path, samp_path = _load_cached(cache_dir, n_expected=n_expected)

    with open(samp_path, "r", encoding="utf-8", errors="ignore") as f:
        samples = [ln.strip() for ln in f if ln.strip()]
    # Align phenotypes to cached genotype sample order
    missing = [s for s in samples if s not in dfp.index]
    if missing:
        raise ValueError(f"Phenotype table missing {len(missing)} samples present in geno cache (example: {missing[:5]})")

    dfp = dfp.loc[samples].copy()

    G_raw = np.load(npy_path, mmap_mode="r")
    meta_raw = pd.read_csv(meta_path)

    if G_raw.shape[0] != len(samples):
        raise ValueError(f"Cache genotype rows ({G_raw.shape[0]}) != sample count ({len(samples)})")

    G_q, meta_q = geno_qc(G_raw, meta_raw, maf_min=MAF_MIN, miss_max=MISS_MAX)

    pc_list = [int(x.strip()) for x in args.pc_list.split(",") if x.strip()]
    race_list = [int(x.strip()) for x in args.include_race.split(",") if x.strip()]
    race_list = [bool(x) for x in race_list]

    addon_dir = out_root / "addons_pc_sweep"
    addon_dir.mkdir(parents=True, exist_ok=True)

    summary_rows: List[Dict] = []

    # Baseline lead under (max PCs in list, include race if available)
    baseline_pcs = max(pc_list) if pc_list else 0
    baseline_race = True if (True in race_list) else False
    baseline_cov = _build_cov(dfp.reset_index(drop=False), baseline_pcs, include_race=baseline_race)

    y = dfp[args.trait].to_numpy(dtype=float)
    baseline_res, baseline_lam, baseline_lead = gwas_ols(y, G_q, meta_q, baseline_cov, block=args.block)
    lead_chr = baseline_lead.get("chr")
    lead_pos = baseline_lead.get("pos")
    if lead_chr is not None and lead_pos is not None:
        lead_mask = (baseline_res["chr"] == lead_chr) & (baseline_res["pos"] == lead_pos)
        lead_rs = baseline_res.loc[lead_mask, "rs"].iloc[0] if ("rs" in baseline_res.columns and lead_mask.any()) else ""
    else:
        lead_rs = ""

    # Save baseline if requested
    if args.save_full:
        baseline_res.to_csv(addon_dir / f"GWAS_{args.trait}_pcs{baseline_pcs}_race{int(baseline_race)}.csv", index=False)

    for npcs in pc_list:
        for use_race in race_list:
            cov = _build_cov(dfp.reset_index(drop=False), npcs, include_race=use_race)
            res, lam, lead = gwas_ols(y, G_q, meta_q, cov, block=args.block)

            # baseline lead stats in this model
            if lead_chr is not None and lead_pos is not None:
                m = (res["chr"] == lead_chr) & (res["pos"] == lead_pos)
                if m.any():
                    lead_logp_here = float(res.loc[m, "-log10p"].iloc[0])
                    rank_here = int(res["-log10p"].rank(ascending=False, method="min").loc[m].iloc[0])
                else:
                    lead_logp_here = float("nan")
                    rank_here = -1
            else:
                lead_logp_here = float("nan")
                rank_here = -1

            summary_rows.append({
                "trait": args.trait,
                "n_pcs": int(npcs),
                "include_race": int(use_race),
                "n_samples": int(len(y)),
                "n_snps_qc": int(res.shape[0]),
                "lambda_gc": float(lam),
                "lead_chr": str(lead.get("chr")),
                "lead_pos": int(lead.get("pos")) if lead.get("pos") is not None else -1,
                "lead_rs": str(lead.get("rs", "")),
                "lead_log10p": float(lead.get("-log10p")),
                "baseline_lead_chr": str(lead_chr),
                "baseline_lead_pos": int(lead_pos) if lead_pos is not None else -1,
                "baseline_lead_rs": str(lead_rs),
                "baseline_lead_log10p_in_this_model": float(lead_logp_here),
                "baseline_lead_rank_in_this_model": int(rank_here),
            })

            if args.save_full:
                res.to_csv(addon_dir / f"GWAS_{args.trait}_pcs{npcs}_race{int(use_race)}.csv", index=False)

    summary_df = pd.DataFrame(summary_rows)
    summary_df.to_csv(addon_dir / "gwas_pc_sweep_summary.csv", index=False)

    print(f"[OK] Wrote add-on outputs to: {addon_dir}")
    print(f"[INFO] Baseline lead used for cross-model tracking: chr={lead_chr}, pos={lead_pos}, rs={lead_rs}")

if __name__ == "__main__":
    main()
